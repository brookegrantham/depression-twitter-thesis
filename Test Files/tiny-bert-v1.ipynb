{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_tweets_df = pd.read_csv('reduced_set')\n",
    "all_tweets_df['label'] = all_tweets_df['label'].replace({2:1})\n",
    "# all_tweets_df = all_tweets_df.dropna(axis=0)\n",
    "# all_tweets_df['condition'] = all_tweets_df['condition'].replace({'depression':0, 'ptsd':1, 'control':2})\n",
    "# all_tweets_df = all_tweets_df[pd.to_numeric(all_tweets_df['condition'], errors='coerce').notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "        Unnamed: 0                                               text  label\n0                0                  This week is going to be perfect~      1\n1                1       @jkpAhe6LeZk Gracias for the follow, ginger!      1\n2                2  @e9nx7G79OxSjgUX but you're mental age is abou...      0\n3                3  @mrpqvocKBHYI  stil gota b there for our teamm...      1\n4                4  About to watch this honey boo boo child show.....      1\n...            ...                                                ...    ...\n113221      113221  @e76GpcvHbF @e_BLroGB8Hl0w @mWEg_jcGU5z6pn4 @c...      0\n113222      113222   @bKlxfsnv2iL6H did you go to the cincinnati one?      0\n113223      113223                                BLACK FUCKING METAL      0\n113224      113224  RT @xSs2zIUeUJ: DRAMA ON THE TL , DRAMA ON THE...      0\n113225      113225    Love work now @lEbc1evVRn6hX back #girlsteam üëØüòÅ      1\n\n[113226 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>This week is going to be perfect~</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>@jkpAhe6LeZk Gracias for the follow, ginger!</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>@e9nx7G79OxSjgUX but you're mental age is abou...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>@mrpqvocKBHYI  stil gota b there for our teamm...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>About to watch this honey boo boo child show.....</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>113221</th>\n      <td>113221</td>\n      <td>@e76GpcvHbF @e_BLroGB8Hl0w @mWEg_jcGU5z6pn4 @c...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>113222</th>\n      <td>113222</td>\n      <td>@bKlxfsnv2iL6H did you go to the cincinnati one?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>113223</th>\n      <td>113223</td>\n      <td>BLACK FUCKING METAL</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>113224</th>\n      <td>113224</td>\n      <td>RT @xSs2zIUeUJ: DRAMA ON THE TL , DRAMA ON THE...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>113225</th>\n      <td>113225</td>\n      <td>Love work now @lEbc1evVRn6hX back #girlsteam üëØüòÅ</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>113226 rows √ó 3 columns</p>\n</div>"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training = 0.7, validation = 0.1, test = 0.2\n",
    "training_tweets, temp_tweets, training_labels, temp_labels = train_test_split(list(all_tweets_df['text']),list(all_tweets_df['label']), test_size=0.3, random_state=123)\n",
    "val_tweets, test_tweets, val_labels, test_labels = train_test_split(temp_tweets, temp_labels, test_size=(1/3), random_state=123)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/vocab.txt from cache at /Users/brookegrantham/.cache/huggingface/transformers/585ac1c3dedc6b808dd35e8770afafe10905d3e723a02617af749d39db780e09.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /Users/brookegrantham/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-tiny')\n",
    "\n",
    "def tokenize_function(dataset):\n",
    "    model_inputs = tokenizer(dataset['text'], padding=\"max_length\", truncation=True, max_length=100)\n",
    "    return model_inputs\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "train = {'text': training_tweets, 'label': training_labels}\n",
    "val = {'text': val_tweets, 'label': val_labels}\n",
    "test = {'text': test_tweets, 'label': test_labels}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train)\n",
    "val_dataset = Dataset.from_dict(val)\n",
    "test_dataset = Dataset.from_dict(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Issue: There was NaN in the dataset not allowing it to be transfered to a Dataset type, thus checking the data condition in rough cells below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# integers = [elm for elm in all_tweets_df['condition'] if not (isinstance(elm, int))]\n",
    "# print(len(integers))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/80 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd48bb7e9d5f47549f137ebed37ec6f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/23 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c809bb4ce10405a9b617ee07b36b32e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/12 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea72b57605de4f55b564bcf4eaaa1416"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"transformer_checkpoints\",\n",
    "    num_train_epochs=1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/config.json from cache at /Users/brookegrantham/.cache/huggingface/transformers/3cf34679007e9fe5d0acd644dcc1f4b26bec5cbc9612364f6da7262aed4ef7a4.a5a11219cf90aae61ff30e1658ccf2cb4aa84d6b6e947336556f887c9828dc6d\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-tiny\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 512,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 2,\n",
      "  \"num_hidden_layers\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/prajjwal1/bert-tiny/resolve/main/pytorch_model.bin from cache at /Users/brookegrantham/.cache/huggingface/transformers/1ee037c9e1a220d5c814779ffe697080d1e6f5b1602e16cf6061aaae41a082c5.038e1aed90492a59d2283f9c44c9fe3ee2380495ff1e7fefb3f1f04af3b685b5\n",
      "Some weights of the model checkpoint at prajjwal1/bert-tiny were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/brookegrantham/Documents/Data Science MSc 2021-2022/PyInt/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 79258\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9908\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='9908' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   2/9908 : < :, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to transformer_checkpoints/checkpoint-500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-1000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-1000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-1500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-1500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-2000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-2000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-2500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-2500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-3000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-3000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-3500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-3500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-4000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-4000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-4500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-4500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-5000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-5000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-5500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-5500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-6000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-6000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-6500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-6500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-7000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-7000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-7500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-7500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-8000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-8000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-8500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-8500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-9000\n",
      "Configuration saved in transformer_checkpoints/checkpoint-9000/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to transformer_checkpoints/checkpoint-9500\n",
      "Configuration saved in transformer_checkpoints/checkpoint-9500/config.json\n",
      "Model weights saved in transformer_checkpoints/checkpoint-9500/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=9908, training_loss=0.6912006567599651, metrics={'train_runtime': 258.2099, 'train_samples_per_second': 306.952, 'train_steps_per_second': 38.372, 'total_flos': 19667238636000.0, 'train_loss': 0.6912006567599651, 'epoch': 1.0})"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('prajjwal1/bert-tiny', num_labels=2)\n",
    "\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def predict_nn(trained_model, test_dataset):\n",
    "\n",
    "    output = trained_model(attention_mask=torch.tensor(test_dataset[\"attention_mask\"]), input_ids=torch.tensor(test_dataset[\"input_ids\"]))\n",
    "\n",
    "    pred_labs = np.argmax(output[\"logits\"].detach().numpy(), axis=1)\n",
    "\n",
    "    gold_labs = test_dataset[\"label\"]\n",
    "\n",
    "    return gold_labs, pred_labs\n",
    "\n",
    "gold_labs, pred_labs = predict_nn(model, test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-tiny on the John Hopkins Twitter dataset:\n",
      "The accuracy score is 0.5428773293296829\n",
      "The f1-score is 0.5482632222028277\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print(\"BERT-tiny on the John Hopkins Twitter dataset:\")\n",
    "print(f'The accuracy score is {accuracy_score(pred_labs, gold_labs)}')\n",
    "print(f'The f1-score is {f1_score(pred_labs,gold_labs)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}